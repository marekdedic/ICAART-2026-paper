\section{\uppercase{A HPO Benchark on Graph Neural Networks}}
\label{sec:benchmark}

In this section, we present a benchmark study to evaluate the performance of different hyperparameter optimization (HPO) methods on Graph Neural Networks (GNNs). We focus on assessing the efficiency and effectiveness of various HPO techniques in optimizing GNN architectures for node classification tasks, which are the most common tasks and are the cornerstones of other techniques for e.g.\ graph classification as well.

\subsection{Experimental Setup}
We conduct our experiments on 9 standard graph datasets. The 3 \enquote{small} datasets were the datasets Cora and CiteSeer \cite{yang_revisiting_2016}, which were used with the \enquote{full} train-test split as in \cite{chen_fastgcn_2018}, and the Squirrel dataset \cite{rozemberczki_multi-scale_2021}. Five medium sized datasets were also used, the PubMed dataset \cite{yang_revisiting_2016}, the DBLP dataset and the full version of the Cora dataset \cite{bojchevski_deep_2018}, the Computers dataset \cite{shchur_pitfalls_2019} and the Flickr dataset \cite{zeng_graphsaint_2019}. Finally, one large dataset was used, the OGB ArXiv dataset \cite{hu_open_2021}.

We evaluate the following HPO methods:
\begin{itemize}
	\item Grid Search
	\item Random Search
	\item Bayesian Optimization \cite{rasmussen_gaussian_2003, snoek_practical_2012}
	\item Sobol Quasi-Monte Carlo \cite{sobol_distribution_1967, bergstra_random_2012}
	\item Tree-structured Parzen Estimator (TPE) \cite{bergstra_algorithms_2011}
\end{itemize}
The methods were all implemented using the Optuna framework \cite{akiba_optuna_2019}. All algorithms run with the default settings, with 2 exceptions: (1) for Sobol, we use the scrambling method of \cite{matousek_l2-discrepancy_1998} and (2) for TPE, we use the multivariate variant \cite{falkner_bohb_2018}.

\subsection{Evaluation Routine}
Each HPO method and dataset combination was evaluated independently 10 times to account for variability in performance due to random initialization and stochastic training processes. For each study, we used 2 stopping criteria to limit the number of trials:
\begin{itemize}
	\item \textbf{Patience}: If no improvement in validation performance was observed for \( M \) consecutive trials, the optimization process was terminated early.
	\item \textbf{Wall-clock time}: Each HPO method was allocated a maximum wall-clock time budget of \( T \) hours to complete its trials.
\end{itemize}
The values of \( M \) and \( T \) were set to ensure a fair comparison across all methods, but were different for small, medium and large datasets to account for the varying training times. Their values are listed in Table~\ref{tab:stopping-criteria}.

\begin{table}
	\caption{HPO stopping criteria based on dataset size.}
	\label{tab:stopping-criteria}
	\centering
	\begin{tabular}{lcc}
		\toprule
		Dataset Size & \( M \) [trials] & \( T \) [hours] \\
		\midrule
		Small        & 30               & 4               \\
		Medium       & 20               & 8               \\
		Large        & 10               & 12              \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Search Spaces}

In our experiments, we used the well-studied and general framework of GraphSAGE \cite{hamilton_inductive_2017} as the base GNN architecture for node classification tasks. We defined a search space encompassing 8 hyperparameters that are critical to the performance of GraphSAGE models. The hyperparameters included in our search space are as follows:
\begin{itemize}
	\item \textbf{Number of layers}: The depth of the GNN.
	\item \textbf{Hidden units per layer}: The number of hidden units in each layer.
	\item \textbf{Activation function}: The non-linear activation function applied after each layer.
	\item \textbf{Optimizer}: The optimization algorithm used for training the model.
	\item \textbf{Dropout rate}: The dropout probability applied to the layers.
	\item \textbf{Aggregation function}: The method used to aggregate neighbor information.
	\item \textbf{\( L_2 \) regularization weight}: The weight of the \( L_2 \) regularization term.
	\item \textbf{Learning rate}: The step size for the optimizer.
\end{itemize}
Table~\ref{tab:search-space} provides a detailed overview of the search space for each dataset. The ranges were chosen based on prior art on GNNs \cite{bronstein_geometric_2021, hamilton_inductive_2017, zeng_graphsaint_2019, zhu_beyond_2020, yang_vqgraph_2023, den_boef_graphpope_2021, li_finding_2022}.

\begin{table*}
	\caption{Hyperparameter search spaces based on dataset sizes.}
	\label{tab:search-space}
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{lcccccccccc}
			\toprule
			\textbf{Dataset} & \textbf{Ep.} & \textbf{Lyrs.} & \textbf{HW} & \textbf{Act.}                                & \textbf{Opt.}                                & \textbf{Drop.}                          & \textbf{Agg.}                              & \textbf{ES} & \textbf{L2}                                       & \textbf{LR}                                          \\
			\midrule
			\multicolumn{11}{l}{\textit{Small datasets}} \\
			\addlinespace
			Cora             & 200          & 1--3           & 16--64      & \multirow{11}{*}{\shortstack{ReLU \\ PReLU}} & \multirow{11}{*}{\shortstack{Adam \\ AdamW}} & \multirow{11}{*}{\shortstack{0 \\ 0.5}} & \multirow{11}{*}{\shortstack{Mean \\ Max}} & 10          & 0, \( 5 \times 10^{-4} \), \( 5 \times 10^{-3} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			CiteSeer         & 200          & 1--3           & 16--64      &                                              &                                              &                                         &                                            & 10          & 0, \( 5 \times 10^{-4} \), \( 5 \times 10^{-3} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			Squirrel         & 500          & 1--3           & 32--128     &                                              &                                              &                                         &                                            & 20          & 0, \( 5 \times 10^{-4} \), \( 5 \times 10^{-3} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			\addlinespace
			\multicolumn{11}{l}{\textit{Medium datasets}} \\
			\addlinespace
			PubMed           & 200          & 1--3           & 16--64      &                                              &                                              &                                         &                                            & 10          & 0, \( 5 \times 10^{-4} \), \( 5 \times 10^{-3} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			CoraFull         & 500          & 2--3           & 64--128     &                                              &                                              &                                         &                                            & 20          & 0, \( 5 \times 10^{-5} \), \( 5 \times 10^{-4} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			DBLP             & 500          & 2--3           & 64--128     &                                              &                                              &                                         &                                            & 20          & 0, \( 5 \times 10^{-5} \), \( 5 \times 10^{-4} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			Computers        & 500          & 2--3           & 64--128     &                                              &                                              &                                         &                                            & 20          & 0, \( 5 \times 10^{-5} \), \( 5 \times 10^{-4} \) & \( 10^{-2} \), \( 5 \times 10^{-3} \), \( 10^{-3} \) \\
			Flickr           & 100          & 2--3           & 128--256    &                                              &                                              &                                         &                                            & 10          & 0, \( 5 \times 10^{-4} \)                         & \( 5 \times 10^{-3} \), \( 10^{-3} \)                \\
			\addlinespace
			\multicolumn{11}{l}{\textit{Large datasets}} \\
			\addlinespace
			ArXiv            & 150          & 2--3           & 256--512    &                                              &                                              &                                         &                                            & 10          & 0, \( 5 \times 10^{-4} \)                         & \( 10^{-2} \), \( 5 \times 10^{-3} \)                \\
			\bottomrule
		\end{tabular}
	}
	\vspace{0.05cm}

	{\footnotesize \textbf{Abbreviations}: \textbf{Ep.}: Epochs, \textbf{Lyrs.}: Number of layers, \textbf{HW}: Hidden Layer Width, \textbf{Act.}: Activation Fn., \textbf{Opt.}: Optimizer, \textbf{Drop.}: Dropout Rate, \textbf{Agg.}: Aggregation Fn., \textbf{ES}: Early Stopping patience, \textbf{L2}: \( L_2 \) Regularization, \textbf{LR}: Learning Rate.}
\end{table*}

\subsection{Results and Analysis}
Table~\ref{tab:benchmark-final-scores} lists the final F1 scores for each HPO method for each dataset, averaged over 10 independent runs. Figure~\ref{fig:benchmark-ranks} then shows the ranks-over-time of the benchmarked HPO methods over the progress of the optimization.

\begin{table*}
	\caption{Final F1 scores for each HPO method and for each dataset, averaged over 10 independent runs. The best method is \textbf{bold} and the second best is \underline{underlined}.}
	\label{tab:benchmark-final-scores}
	\centering
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Dataset} & \textbf{Random}    & \textbf{Grid} & \textbf{BO}        & \textbf{TPE}       & \textbf{QMC}       \\
		\midrule
		Cora             & 0.8560             & 0.8491        & \textbf{0.8747}    & \underline{0.8659} & 0.8351             \\
		CiteSeer         & 0.7146             & 0.7046        & \underline{0.7172} & \textbf{0.7236}    & 0.7089             \\
		Squirrel         & \underline{0.3659} & 0.3605        & 0.3544             & \textbf{0.3755}    & 0.3549             \\
		PubMed           & 0.8515             & 0.8457        & \textbf{0.8825}    & \underline{0.8643} & 0.8596             \\
		CoraFull         & 0.6211             & 0.6371        & \underline{0.6450} & \textbf{0.6555}    & 0.6385             \\
		DBLP             & 0.8051             & 0.7996        & \textbf{0.8118}    & 0.8085             & \underline{0.8088} \\
		Computers        & 0.6973             & 0.5999        & \textbf{0.8945}    & \underline{0.8047} & 0.7428             \\
		Flickr           & 0.0864             & 0.0961        & \textbf{0.1908}    & \underline{0.1460} & 0.1069             \\
		ArXiv            & 0.3950             & 0.3796        & \underline{0.3987} & \textbf{0.4098}    & 0.3955             \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{figure*}
	\centering
	\resizebox{\linewidth}{!}{%
		\input{images/benchmark_ranks/benchmark_ranks.pgf}
	}
	\caption{Ranks-over-time of the benchmarked HPO methods over the progress of the optimization.}
	\label{fig:benchmark-ranks}
\end{figure*}

Three broad observations can be made from the results.
\begin{itemize}
	\item SMBO methods (BO and TPE) outperform non-SMBO methods (Random, Grid, QMC) consistently across all datasets. Only on the Squirrel and DBLP datasets does any other method reach the second best performance, otherwise SMBO methods take the top two spots. However, between BO and TPE there is no clear winner, with both methods achieving the best performance on multiple datasets. It is also worth noting that the advantage of SMBO methods only becomes pronounced after 10 trials, which is the startup period needed to build an initial model of the search space.
	\item Simple methods like random search and grid search remain solid choices for HPO as they usually trail behind SMBO methods by only a small margin. The exceptions to this are the Computers and Flickr datasets, where the performance gap is significant.
	\item Sobol QMC doesn't fulfill its promise of better coverage of the search space, as it consistently ranks worse than SMBO methods and on smaller datasets also worse than random search.
\end{itemize}
