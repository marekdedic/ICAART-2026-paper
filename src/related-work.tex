\section{\uppercase{Related work}}\label{sec:related-work}

Classical HPO approaches range from simple, exhaustive grid search to more efficient, non-exhaustive sampling methods like random search, which was shown to be surprisingly effective, and Quasi-Monte Carlo (QMC) sampling \cite{bergstra_random_2012}. More sophisticated strategies leverage sequential model-based optimization (SMBO). Prominent SMBO approaches include Bayesian Optimization (BO), which often uses Gaussian Processes \cite{snoek_practical_2012}, and Tree-structured Parzen Estimators (TPE) \cite{bergstra_algorithms_2011}. These methods build a surrogate model of the objective function and use an acquisition function to intelligently select the next set of hyperparameters to evaluate, balancing exploration and exploitation.

In parallel, Graph Neural Networks (GNNs) have emerged as the state-of-the-art method for machine learning on graph-structured data \cite{kipf_semi-supervised_2017}. GNNs, however, introduce a significant number of their own hyperparameters. These include not only standard training parameters like learning rate and weight decay but also critical architectural parameters such as the number of layers, the type of aggregation function (e.g., GCN, GAT, GraphSAGE), and the dimensionality of hidden representations.

The intersection of HPO and graph learning is a nascent but growing field, largely motivated by the high cost of GNN training and the complex interplay of their hyperparameters. Much of the effort has been concentrated under the umbrella of Neural Architecture Search (NAS) for GNNs. For example, GraphNAS \cite{gao_graph_2020} and Auto-GNN \cite{zhou_auto-gnn_2022} employed reinforcement learning to search for optimal GNN architectures. Both of these works, however, primarily focus on discovering new architectures rather than tuning hyperparameters of existing ones and evaluate on a limited number of datasets.

Works specifically addressing HPO for GNNs are most often limited to specific domains, such as molecular property prediction \cite{yuan_genetic_2021, yuan_systematic_2021, yuan_which_2021, ebadi_hyperparameter_2025} or mRNA degradation prediction \cite{vodilovska_hyperparameter_2023}. These studies often evaluate a narrow selection of datasets and don't offer a comprehensive comparison to other HPO methods, making it difficult to generalize their findings across different types of graph data and GNN architectures.
