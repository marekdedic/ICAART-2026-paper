\section{\uppercase{Introduction}}

Machine learning (ML) has become a cornerstone of modern technology. Many ML applications naturally involve structured data—such as network flows, system call trees, or privilege graphs—that are most faithfully modeled as graphs. A critical, though often overlooked, aspect of deploying these models is hyperparameter optimization (HPO). The specific choices made for hyperparameters -- such as learning rate, regularization strength, hidden dimensions, and sampling strategies -- can cause significant performance swings. While simple classical HPO methods, such as grid search and random search, remain popular for their ease of use, more sophisticated strategies promise greater efficiency. These advanced strategies include Gaussian-process Bayesian optimization (GP), the Tree-structured Parzen Estimator (TPE), and Quasi-Monte Carlo (QMC) sampling (specifically Sobol QMC), which work by modeling or more uniformly covering the search space.

Despite extensive benchmarking of HPO algorithms on traditional image and text tasks, there has been no systematic study of how these techniques perform on general graph-structured problems. This work addresses that gap by presenting the first systematic HPO benchmark on Graph Neural Networks (GNNs).

The key contributions of this research are:
\begin{enumerate}
	\item \textbf{Systematic HPO Benchmark on GNNs}: The study evaluates and analyzes the convergence speed, final F1 score, and robustness of five popular HPO methods (grid, random, GP, TPE, and Sobol QMC) for tuning GraphSAGE across ten diverse graph datasets. This establishes a reproducible baseline for future work on GNN hyperparameter tuning.
	\item \textbf{Meta-Learning for Hyperparameter Transfer}: The work evaluates a cross-dataset Random Forest (RF) surrogate model that learns how dataset properties (meta-features) interact with hyperparameter performance. This model leverages past HPO runs on related graphs to warm-start the search on new datasets, demonstrating its ability to accelerate tuning and reduce the number of costly model evaluations required to reach peak performance.
\end{enumerate}

\todo{Check}
The remainder of the paper details the methodological framework, introduces the Cross-RF HPO algorithm under study, presents the benchmark results, and evaluates the effectiveness of the proposed meta-learning approach for hyperparameter transfer across graph datasets.
