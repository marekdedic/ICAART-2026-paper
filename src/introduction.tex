\section{\uppercase{Introduction}}

Machine learning (ML) has become a cornerstone of modern technology. Many ML applications naturally involve structured data—such as network flows, system call trees, or privilege graphs—that are most faithfully modeled as graphs. A critical, though often overlooked, aspect of deploying these models is hyperparameter optimization (HPO). The specific choices made for hyperparameters -- such as learning rate, regularization strength, hidden dimensions, and sampling strategies -- can cause significant performance swings. While simple classical HPO methods, such as grid search and random search, remain popular for their ease of use, more sophisticated strategies promise greater efficiency. These advanced strategies include BO, TPE, and QMC sampling, which work by modeling or more uniformly covering the search space.

\subsection{Contributions}

Despite extensive benchmarking of HPO algorithms on traditional tabular, image and text tasks, existing works evaluating HPO methods for graph learning are limited in scope (see Section~\ref{sec:related-work}). They typically focus on a small number of graph datasets, usually in a single domain, a limited set of hyperparameters, or a limited set of HPO methods. This work addresses that gap by presenting the first systematic HPO benchmark on Graph Neural Networks (GNNs).

Additionally, we propose a novel approach to HPO on graph datasets specifically. This approach leverages the fact that (unlike most other dataset types) graph datasets have an extensive internal structure that can be characterized by various statistical properties (e.g., number of nodes, edges, degree distribution, homophily, etc.). These properties can provide valuable insights into the nature of the data and its underlying relationships. Our work aims to exploit these unique characteristics of graph datasets to enhance the HPO process. Specifically, we argue that these graph-specific properties can be used to inform and guide the HPO process more effectively than generic methods that do not consider the structure of the data.

The key contributions of this research are:
\begin{enumerate}
	\item \textbf{Systematic HPO Benchmark on GNNs}: The study evaluates and analyzes the convergence speed and final score, of five popular HPO methods (grid search, random search, BO, TPE, and Sobol QMC) for tuning GraphSAGE across nine diverse graph datasets. This establishes a reproducible baseline for future work on GNN hyperparameter tuning.
	\item \textbf{Meta-Learning for Hyperparameter Transfer}: The work evaluates a cross-dataset RF surrogate model that learns how dataset properties (meta-features) interact with hyperparameter performance. This model leverages past HPO runs on related graphs to warm-start the search on new datasets, demonstrating its ability to accelerate tuning and reduce the number of costly model evaluations required to reach peak performance.
\end{enumerate}

In Section~\ref{sec:related-work}, we review related work on HPO methods and existing benchmarks, particularly in the context of graph learning. Section~\ref{sec:context} provides necessary context of HPO more generally. Section~\ref{sec:cross-rf} introduces the Cross-RF method for HPO for graph learning. Section~\ref{sec:benchmark} outlines our benchmarking methodology, including dataset selection, hyperparameter spaces, and evaluated methods and reports on the performance of SoTA HPO methods in the graph setting. In Section~\ref{sec:experimental-evaluation}, we evaluate the Cross-RF method, comparing its performance to established HPO methods.Finally, Section~\ref{sec:conclusion} summarizes our findings and discusses future research directions.
