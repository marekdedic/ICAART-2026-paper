\section{\uppercase{The Cross-RF Method for Hyper-Parameter Optimization on Graph Datasets}}

In this section, we introduce the Cross-RF method, a novel approach for hyper-parameter optimization specifically designed for graph datasets.

Suppose we have a graph property descriptor function \( \mathcal{P}: \mathcal{D} \rightarrow \mathvec{d} \) that maps a graph dataset \( \mathcal{D} \) to a vector of numerical properties \( \mathvec{d} \) (e.g., number of nodes, number of edges, average degree, clustering coefficient, etc.). This function allows us to capture the characteristics of different graph datasets in a structured manner. Suppose we also have a so-called \name{meta-model} \( \mathcal{M}_\rho \) that takes as input the properties of a graph dataset \( \mathcal{P} \left( \mathcal{D} \right) \) and a hyper-parameter configuration \( \lambda \), and outputs an estimate of a performance metric \( \rho \) (e.g., accuracy, F1-score) of a machine learning model trained on \( \mathcal{D} \) with the hyper-parameter configuration \( \lambda \). Using this, we can define the Cross-RF hyper-parameter tuning method as follows:
\begin{equation*}
	\tau \left( \mathcal{D}, \mathscr{F}, \tilde{\Lambda}, \rho \right) = \argmax_{\lambda \in \tilde{\Lambda}} \mathcal{M}_\rho \left( \mathcal{P} \left( \mathcal{D} \right), \lambda \right)
\end{equation*}
where:
\begin{itemize}
	\item \( \mathcal{D} \) is the target graph dataset for which we want to optimize hyper-parameters.
	\item \( \mathscr{F} \) is an inducer function that maps hyper-parameter configurations to machine learning models.
	\item \( \tilde{\Lambda} \) is a predefined set of hyper-parameter configurations to be evaluated.
	\item \( \rho \) is the performance metric we aim to optimize (i.e.\ the outer loss).
\end{itemize}
The feasibility of the Cross-RF method relies on the construction of the meta-model \( \mathcal{M}_\rho \) and on its computational complexity -- namely, the meta-model must be fast enough so that evaluating it on every hyper-parameter configuration from \( \tilde{\Lambda} \) at every step of the optimization isn't prohibitively costly.

To build the meta-model \( \mathcal{M}_\rho \), we will use a standard simple regression model (in our case, a Random Forest Regressor, but other regression models could be used as well) trained on a meta-dataset \( \mathcal{H} \).
The meta-dataset \( \mathcal{H} \) is initialized as \( \mathcal{H} = \emptyset \) and constructed by collecting dataset properties, hyper-parameter configurations, and corresponding performance metrics across the progress of the training:
\begin{equation*}
	\begin{split}
		\mathcal{H} = \biggl\{ &\left( \mathcal{P} \left( D^{(1)} \right), \lambda^{(1)}, \rho( \mathvec{y}_{\mathcal{D}^{(1)}}, \hat{f}_{\lambda^{(1)}} \left( \mathmat{X}_{\mathcal{D}^{(1)}} \right) ) \right), \\
		&\left( \mathcal{P} \left( D^{(2)} \right), \lambda^{(2)}, \rho( \mathvec{y}_{\mathcal{D}^{(2)}}, \hat{f}_{\lambda^{(2)}} \left( \mathmat{X}_{\mathcal{D}^{(2)}} \right) ) \right), \dots \biggr\}
	\end{split}
\end{equation*}
An observant reader may have already noticed that in such a construction of the meta-dataset \( \mathcal{H} \), the dataset properties \( \mathcal{P} \left( D^{(i)} \right) \) will all be identical and thus of no benefit to the meta-model. To fix this issue, we must employ a cross-dataset pre-training strategy, where we collect data from multiple different graph datasets \( \mathcal{D}^{(1)}, \mathcal{D}^{(2)}, \dots, \mathcal{D}^{(n)} \) to populate the starting meta-dataset \( \mathcal{H} \). This way, the meta-model \( \mathcal{M}_\rho \) can learn to generalize across different graph datasets based on their properties. The datasets used in the pre-training phase should ideally be diverse and representative of the types of graphs we expect to encounter in practice and should, crucially for the validity of the evaluation of the Cross-RF method, not contain the dataset \( \mathcal{D} \) on which the method is ultimately evaluated.

\subsection{Dataset Property Descriptor}

\todo{Review the table and the next par}
Table~\ref{tab:graph-properties} lists the graph dataset properties considered in our experiments. These properties were selected based on their relevance to graph structure, node attributes, and the specific task at hand (e.g., node classification). The properties encompass a range of characteristics, from basic metrics like node count and average degree to more complex measures such as assortativity and homophily. By incorporating these diverse properties into the descriptor function \( \mathcal{P} \), we aim to capture a comprehensive representation of each graph dataset, which is essential for the effective training of the meta-model \( \mathcal{M}_\rho \).

\begin{table*}[h]
	\caption{Graph dataset properties considered.}
	\label{tab:graph-properties}
	\centering
	\begin{tabularx}{\linewidth}{XcccX}
		\toprule
		\multirow{2}{*}{\textbf{Graph Property}}           & \multicolumn{3}{c}{\textbf{Awareness}}                   & \multirow{2}{*}{\textbf{Description / Definition}} \\
		\cmidrule(lr){2-4}
		                                                   & \textbf{Task} & \textbf{Structure} & \textbf{Attributes} & \\
		\midrule
		\textbf{Node count}                                & \crossmark    & \crossmark         & \crossmark          & Number of nodes. \\
		\textbf{Edge count}                                & \crossmark    & \crossmark         & \crossmark          & Number of edges. \\
		\textbf{Number of components}                      & \crossmark    & \checkmark         & \crossmark          & Number of connected components of the graph. \\
		\textbf{Average node degree}                       & \crossmark    & \checkmark         & \crossmark          & Average node degree in the graph. \\
		\textbf{Global assortativity}                      & \crossmark    & \checkmark         & \crossmark          & Measure of the tendency of nodes to connect with other similar nodes, rather than dissimilar nodes~[9]. \\
		\textbf{Attribute similarity}                      & \crossmark    & \checkmark         & \checkmark          & Average cosine similarity of attributes across all edges in the graph. \\
		\textbf{Attribute homophily}                       & \checkmark    & \checkmark         & \crossmark          & Measure of how clustered together are nodes with similar attributes. \\
		\textbf{Edge homophily}                            & \checkmark    & \checkmark         & \crossmark          & Fraction of edges connecting nodes of the same class. \\
		\textbf{Node homophily}                            & \checkmark    & \checkmark         & \crossmark          & Fraction of node neighbours having the same class as the node in question, averaged over all nodes. \\
		\textbf{Class homophily}                           & \checkmark    & \checkmark         & \crossmark          & A modification of node homophily that is invariant to the number of classes. \\
		\textbf{Ratio of positive nodes of degree $>1$}    & \checkmark    & \checkmark         & \crossmark          & Ratio of positive nodes with degree greater than one. \\
		\textbf{Fraction of positive nodes of degree $>2$} & \checkmark    & \checkmark         & \crossmark          & The fraction of positive nodes with degree greater than two, out of those with degree greater than one. \\
		\textbf{Average positive node degree}              & \checkmark    & \checkmark         & \crossmark          & Average node degree in the sub-graph restricted to nodes from $V^1$. \\
		\textbf{Relative presence of positive edges}       & \checkmark    & \checkmark         & \crossmark          & Number of edges connecting positive nodes, divided by the number of edges that would be present in a theoretical clique constructed of all positive nodes. \\
		\textbf{Positive attribute similarity}             & \checkmark    & \crossmark         & \checkmark          & Avg.\ feature similarity within positive class. \\
		\textbf{Positive to negative attribute similarity} & \checkmark    & \crossmark         & \checkmark          & Avg.\ feature similarity of positive vs negative nodes. \\
		\textbf{Negative to positive attribute similarity} & \checkmark    & \crossmark         & \checkmark          & Avg.\ feature similarity of negative vs positive nodes. \\
		\textbf{Adjusted homophily}                        & \checkmark    & \checkmark         & \crossmark          & Homophily adjusted for degree–class distribution. \\
		\textbf{Node label informativeness}                & \checkmark    & \checkmark         & \crossmark          & Mutual‐information‐based measure of how informative node labels are about the graph’s connectivity. \\
		\textbf{Balanced accuracy}                         & \checkmark    & \checkmark         & \crossmark          & Class‐averaged fraction of edges connecting nodes to same‐class neighbors. \\
		\textbf{Adjusted accuracy}                         & \checkmark    & \checkmark         & \crossmark          & Accuracy metric adjusted to range $[0,1]$ based on balanced accuracy. \\
		\bottomrule
	\end{tabularx}
\end{table*}
