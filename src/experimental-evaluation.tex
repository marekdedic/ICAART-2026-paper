\section{\uppercase{Experimental evaluation of the Cross-RF method}}
\label{sec:experimental-evaluation}

In order to evaluate the proposed Cross-RF method, we conducted experiments on the same datasets as described in Section~\ref{sec:benchmark}, using the same evaluation routine and search space. For better readability, we limit our comparison to only the two highest-performing reference methods from Section~\ref{sec:benchmark}, which are Bayesian Optimisation (BO) and Tree Parzen Estimators (TPE).

\subsection{Meta-model architecture}

The metamodel used in the Cross-RF method is a Random Forest regressor \cite{breiman_random_2001}, implemented using the \texttt{RandomForestRegressor} class from the \texttt{scikit-learn} library \cite{pedregosa_scikit-learn_2011}. The Random Forest was configured with 100 trees, and other hyper-parameters were set to their default values, except for the maximum number of features to consider for each split, which was set to \( 30\% \) of the total number of features.

Several small implementation details are worth mentioning. First, in each step of the optimisation, all of the already evaluated configurations from \( \mathcal{H} \) are excluded from the set of candidate configurations \( \tilde{\Lambda} \), so that the same configuration isn't needlessly re-evaluated. Second, all categorical features are one-hot encoded before being passed to the Random Forest model. Finally, in all our experiments, we used the F1-score as the performance metric \( \rho \) to be optimised.

For the construction of the metadataset \( \mathcal{H} \) for a Cross-RF model trained on a dataset \( \mathcal{D} \), we used the remaining 8 datasets from the benchmark, excluding \( \mathcal{D} \).

\subsection{Results and Analysis}
Table~\ref{tab:cross-rf-final-scores} lists the final F1 scores for Cross-RF and the 2 reference methods for each dataset, averaged over 10 independent runs. Figure~\ref{fig:cross-rf-ranks} then shows the ranks-over-time of the methods over the progress of the optimization.

\begin{table}
	\caption{Final F1 scores for each HPO method and for each dataset, averaged over 10 independent runs. The best method is \textbf{bold} and the second best is \underline{underlined}.}
	\label{tab:cross-rf-final-scores}
	\centering
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Dataset} & \textbf{BO}        & \textbf{TPE}       & \textbf{Cross-RF}  \\
		\midrule
		Cora             & \textbf{0.8747}    & 0.8659             & \underline{0.8727} \\
		CiteSeer         & \underline{0.7172} & \textbf{0.7236}    & 0.7095             \\
		Squirrel         & 0.3544             & \underline{0.3755} & \textbf{0.3769}    \\
		PubMed           & \textbf{0.8825}    & 0.8643             & \underline{0.8807} \\
		CoraFull         & \underline{0.6450} & \textbf{0.6555}    & 0.6426             \\
		DBLP             & \underline{0.8118} & 0.8085             & \textbf{0.8123}    \\
		Computers        & \underline{0.8945} & 0.8047             & \textbf{0.9023}    \\
		Flickr           & \underline{0.1908} & 0.1460             & \textbf{0.1956}    \\
		ArXiv            & 0.3987             & \textbf{0.4098}    & \underline{0.4094} \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure*}
	\centering
	\resizebox{\linewidth}{!}{%
		\input{images/cross-rf_ranks/cross-rf_ranks.pgf}
	}
	\caption{Ranks-over-time of Cross-RF and reference methods over the progress of the optimization.}
	\label{fig:cross-rf-ranks}
\end{figure*}

From Table~\ref{tab:cross-rf-final-scores}, we can see that Cross-RF achieves the best final performance on 4 out of the 9 datasets, while being second best on another 3 datasets. Only on the CiteSeer and CoraFull datasets does Cross-RF perform worse than both reference methods. Overall, Cross-RF achieves an average rank of 1.78, outperforming both BO (2.0) and TPE (2.22).

While Figure~\ref{fig:cross-rf-ranks} contains a lot of noise, we can broadly observe that Cross-RF starts off strong, achieving the best average rank in the early stages of the optimization, before BO and TPE pass the startup phase. As the optimization progresses, Cross-RF seems to be overtaken by BO (however, this is a very weak observation due to the noise), until finally, at the end of the optimization, Cross-RF regains its lead. This is broadly in line with the design of the Cross-RF method, which can at first leverage knowledge from other datasets to quickly find promising configurations, before relying on evaluations on the target dataset to refine its model and find the best configurations for it specifically.
