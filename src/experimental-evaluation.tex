\section{\uppercase{Experimental evaluation of the Cross-RF method}}
\label{sec:experimental-evaluation}

In order to evaluate the proposed Cross-RF method, we conducted experiments on the same datasets as described in Section~\ref{sec:benchmark}, using the same evaluation routine and search space.

\subsection{Meta-model architecture}

The metamodel used in the Cross-RF method is a Random Forest regressor \cite{breiman_random_2001}, implemented using the \texttt{RandomForestRegressor} class from the \texttt{scikit-learn} library \cite{pedregosa_scikit-learn_2011}. The Random Forest was configured with 100 trees, and other hyper-parameters were set to their default values, except for the maximum number of features to consider for each split, which was set to \( 30\% \) of the total number of features.

Several small implementation details are worth mentioning. First, in each step of the optimisation, all of the already evaluated configurations from \( \mathcal{H} \) are excluded from the set of candidate configurations \( \tilde{\Lambda} \), so that the same configuration isn't needlessly re-evaluated. Second, all categorical features are one-hot encoded before being passed to the Random Forest model. Finally, in all our experiments, we used the F1-score as the performance metric \( \rho \) to be optimised.

For the construction of the metadataset \( \mathcal{H} \) for a Cross-RF model trained on a dataset \( \mathcal{D} \), we used the remaining 8 datasets from the benchmark, excluding \( \mathcal{D} \).

\subsection{Results and Analysis}
\todo[inline]{Results}

\begin{table*}
	\caption{Final F1 scores for each HPO method and for each dataset, averaged over 10 independent runs. The best method is \textbf{bold} and the second best is \underline{underlined}.}
	\label{tab:cross-rf-final-scores}
	\centering
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Dataset} & \textbf{BO}        & \textbf{TPE}       & \textbf{Cross-RF}  \\
		\midrule
		Cora             & \textbf{0.8747}    & 0.8659             & \underline{0.8727} \\
		CiteSeer         & \underline{0.7172} & \textbf{0.7236}    & 0.7095             \\
		Squirrel         & 0.3544             & \underline{0.3755} & \textbf{0.3769}    \\
		PubMed           & \textbf{0.8825}    & 0.8643             & \underline{0.8807} \\
		CoraFull         & \underline{0.6450} & \textbf{0.6555}    & 0.6426             \\
		DBLP             & \underline{0.8118} & 0.8085             & \textbf{0.8123}    \\
		Computers        & \underline{0.8945} & 0.8047             & \textbf{0.9023}    \\
		Flickr           & \underline{0.1908} & 0.1460             & \textbf{0.1956}    \\
		ArXiv            & 0.3987             & \textbf{0.4098}    & \underline{0.4094} \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{figure*}
	\centering
	\resizebox{\linewidth}{!}{%
		\input{images/cross-rf_ranks/cross-rf_ranks.pgf}
	}
	\caption{Ranks-over-time of Cross-RF and reference methods over the progress of the optimization.}
	\label{fig:cross-rf-ranks}
\end{figure*}
