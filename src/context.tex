\section{\uppercase{Wider context and Notation}}
\label{sec:context}

Most machine learning algorithms, including graph neural networks (GNNs), have hyperparameters that need to be set before training the model. The process of finding the optimal hyperparameter configuration is known as hyperparameter optimization (HPO). While previously manually tuning hyperparameters was common, it is now widely recognized that systematic HPO methods lead to better model performance and generalization. HPO is essentially a black-box optimization problem, moreover, it is a very expensive one, as each evaluation of a hyperparameter configuration requires training and validating a machine learning model.

In this work, we mainly consider HPO in the context of supervised learning tasks. We mostly follow the notation from \cite{bischl_hyperparameter_2023}. Let \( \mathcal{D} \) be a labelled dataset of pairs \( \left( \mathvec{x}_i, y_i \right) \), where \( \mathvec{x}_i \in \mathcal{X} \) are the input features and \( y_i \in \mathcal{Y} \) are the corresponding labels. We assume that \( \mathcal{D} \) is drawn as repeated i.i.d.\ samples from an unknown joint probability distribution \( \mathrm{P}_{\mathcal{X}\mathcal{Y}} \). We may sometimes want to operate with the whole dataset at once, in which case we denote by \( \mathmat{X}_\mathcal{D} \) the matrix of all input features and by \( \mathvec{y}_\mathcal{D} \) the vector of all labels.

The goal of supervised learning is to learn a function \( f: \mathcal{X} \to \mathfield{R}^g \) that maps input features to predictions (we consider \( g \) to be the number of classes for classification and \( g = 1 \) for regression) and generalizes well to unseen data. The function space to which a model \( f \) belongs is usually called the \name{hypothesis space} and is denoted \( \mathscr{H} \). The process of obtaining such a model is configured by some hyperparameters forming a hyperparameter configuration \( \lambda \in \boldsymbol\Lambda \). Formally, the learning process can be described as an application of an \name{inducer} function \( \mathscr{F} : \mathrm{D} \times \boldsymbol\Lambda \to \mathscr{H} \), which takes as input a dataset \( \mathcal{D} \in \mathrm{D} \) and a hyperparameter configuration \( \lambda \in \boldsymbol\Lambda \), and outputs a trained model \( \hat{f} = \mathscr{F} ( \mathcal{D}, \lambda ) \) (we may sometimes denote this model \( \hat{f}_\lambda \) to emphasize the role of the hyperparameter configuration in its training), where \( \mathrm{D} = \bigcup_{n \in \mathfield{N}} \left( \mathcal{X}, \mathcal{Y} \right)^n \) is the set of all datasets.

After training a model \( \hat{f} \), we need to evaluate its performance give new, unseen data. We define a general performance measure \( \rho : \bigcup_{m \in \mathfield{N}} \left( \mathcal{Y}^m \times \mathfield{R}^{g \times m} \right) \to \mathfield{R} \) that takes as input the true labels and the model predictions for \( m \) samples and outputs a real-valued score (for example, accuracy for classification or mean squared error for regression).

The ML learner \( \mathscr{F} \) is usually parameterized by a set of hyperparameters \( \lambda \in \boldsymbol\Lambda \). The goal of HPO is to find the optimal hyperparameter configuration \( \lambda^* \) that maximizes the performance measure \( \rho \) on unseen data. Usually, a HPO algorithm considers only a finite set of hyperparameter configurations \( \tilde{\Lambda} \subset \boldsymbol\Lambda \) called a \name{search space} due to computational constraints. Most HPO algorithms work by iteratively selecting hyperparameter configurations from the search space, training models using these configurations, and evaluating their performance using the performance measure \( \rho \). Based on the evaluation results, the HPO algorithm updates its strategy for selecting the next hyperparameter configurations to evaluate. This process allows such an HPO algorithm to be formalized as a tuner \( \tau : \left( \mathcal{D}, \mathscr{F}, \tilde{\Lambda}, \rho \right) \to \hat\lambda \), which takes as input a dataset \( \mathcal{D} \), an inducer \( \mathscr{F} \), a search space \( \tilde{\Lambda} \), and a performance measure \( \rho \), and proposes an estimate \( \hat{\lambda} \in \tilde{\Lambda} \) of the optimal hyperparameter configuration \( \lambda^* \).

\todo[inline]{Maybe also discuss common HPO methods here?}
