
@article{ebadi_hyperparameter_2025,
	title = {Hyperparameter optimization and neural architecture search algorithms for graph {Neural} {Networks} in cheminformatics},
	volume = {254},
	issn = {0927-0256},
	url = {https://www.sciencedirect.com/science/article/pii/S0927025625002472},
	doi = {10.1016/j.commatsci.2025.113904},
	abstract = {Cheminformatics, an interdisciplinary field bridging chemistry and information science, leverages computational tools to analyze and interpret chemical data, playing a critical role in drug discovery, material science, and environmental chemistry. Traditional methods, reliant on rule-based algorithms and expert-curated datasets, face challenges in scalability and adaptability. Recently, machine learning and deep learning have revolutionized cheminformatics by offering data-driven approaches that uncover complex patterns in vast chemical datasets, advancing molecular property prediction, chemical reaction modeling, and de novo molecular design. Among the most promising techniques are Graph Neural Networks (GNNs), which have emerged as a powerful tool for modeling molecules in a manner that mirrors their underlying chemical structures. Despite their success, the performance of GNNs is highly sensitive to architectural choices and hyperparameters, making optimal configuration selection a non-trivial task. Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) are crucial for improving GNN performance, but the complexity and computational cost of these processes have traditionally hindered progress. This review examines various strategies for automating NAS and HPO in GNNs, highlighting their potential to enhance model performance, scalability, and efficiency in key cheminformatics applications. As the field evolves, automated optimization techniques are expected to play a pivotal role in advancing GNN-based solutions in cheminformatics.},
	urldate = {2025-10-20},
	journal = {Computational Materials Science},
	author = {Ebadi, Ali and Kaur, Manpreet and Liu, Qian},
	month = may,
	year = {2025},
	keywords = {Cheminformatics, Deep learning (DL), Graph neural networks (GNN), Hyperparameter optimization (HPO), Machine learning (ML), Neural architecture search (NAS)},
	pages = {113904},
}

@inproceedings{yuan_which_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {Which hyperparameters to optimise? an investigation of evolutionary hyperparameter optimisation in graph neural network for molecular property prediction},
	isbn = {978-1-4503-8351-6},
	shorttitle = {Which hyperparameters to optimise?},
	url = {https://doi.org/10.1145/3449726.3463192},
	doi = {10.1145/3449726.3463192},
	abstract = {Most GNNs for molecular property prediction are proposed based on the idea of learning the representations for the nodes by aggregating the information of their neighbour nodes in graph layers. Then, the representations can be passed to subsequent task-specific layers to deal with individual downstream tasks. Facing real-world molecular problems, the hyperparameter optimisation for those layers are vital. In this research, we focus on the impact of selecting two types of GNN hyperparameters, those belonging to graph layers and those of task-specific layers, on the performance of GNN for molecular property prediction. In our experiments, we employed a state-of-the-art evolutionary algorithm (i.e., CMA-ES) for HPO. The results reveal that optimising the two types of hyperparameters separately can improve GNNs' performance, but optimising both types of hyperparameters simultaneously will lead to predominant improvements.},
	urldate = {2025-10-20},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jul,
	year = {2021},
	pages = {1403--1404},
}

@inproceedings{yuan_genetic_2021,
	title = {A {Genetic} {Algorithm} with {Tree}-structured {Mutation} for {Hyperparameter} {Optimisation} of {Graph} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/abstract/document/9504717},
	doi = {10.1109/CEC45853.2021.9504717},
	abstract = {In recent years, graph neural networks (GNNs) have gained increasing attention, as they possess the excellent capability of processing graph-related problems. In practice, hyperparameter optimisation (HPO) is critical for GNNs to achieve satisfactory results, but this process is costly because the evaluations of different hyperparameter settings require excessively training many GNNs. Many approaches have been proposed for HPO, which aims to identify promising hyperparameters efficiently. In particular, the genetic algorithm (GA) for HPO has been explored, which treats GNNs as a black-box model, of which only the outputs can be observed given a set of hyperparameters. However, because GNN models are sophisticated and the evaluations of hyperparameters on GNNs are expensive, GA requires advanced techniques to balance the exploration and exploitation of the search and make the optimisation more effective given limited computational resources. Therefore, we proposed a tree-structured mutation strategy for GA to alleviate this issue. Meanwhile, we reviewed the recent HPO works, which gives room for the idea of tree-structure to develop, and we hope our approach can further improve these HPO methods in the future.},
	urldate = {2025-10-20},
	booktitle = {2021 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jun,
	year = {2021},
	keywords = {Computational modeling, Evolutionary computation, generic algorithm, Genetic algorithms, graph neural network, Graph neural networks, hyperparameter optimisation, Optimization, Training, tree-structured mutation},
	pages = {482--489},
}

@inproceedings{yuan_systematic_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {A systematic comparison study on hyperparameter optimisation of graph neural networks for molecular property prediction},
	isbn = {978-1-4503-8350-9},
	url = {https://dl.acm.org/doi/10.1145/3449639.3459370},
	doi = {10.1145/3449639.3459370},
	abstract = {Graph neural networks (GNNs) have been proposed for a wide range of graph-related learning tasks. In particular, in recent years, an increasing number of GNN systems were applied to predict molecular properties. However, a direct impediment is to select appropriate hyperparameters to achieve satisfactory performance with lower computational cost. Meanwhile, many molecular datasets are far smaller than many other datasets in typical deep learning applications. Most hyperparameter optimization (HPO) methods have not been explored in terms of their efficiencies on such small datasets in the molecular domain. In this paper, we conducted a theoretical analysis of common and specific features for two state-of-the-art and popular algorithms for HPO: TPE and CMA-ES, and we compared them with random search (RS), which is used as a baseline. Experimental studies are carried out on several benchmarks in MoleculeNet, from different perspectives to investigate the impact of RS, TPE, and CMA-ES on HPO of GNNs for molecular property prediction. In our experiments, we concluded that RS, TPE, and CMA-ES have their individual advantages in tackling different specific molecular problems. Finally, we believe our work will motivate further research on GNN as applied to molecular machine learning problems in chemistry and materials sciences.},
	urldate = {2025-10-20},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jun,
	year = {2021},
	pages = {386--394},
}

@inproceedings{vodilovska_hyperparameter_2023,
	title = {Hyperparameter {Optimization} of {Graph} {Neural} {Networks} for {mRNA} {Degradation} {Prediction}},
	url = {https://ieeexplore.ieee.org/abstract/document/10159737},
	doi = {10.23919/MIPRO57284.2023.10159737},
	abstract = {Graph Neural Networks (GNN) emerged as increasingly attractive deep learning models for complex data, making them extremely useful in biochemical and pharmaceutical domains. However, building a good-performing GNN requires lots of parameter choices and Hyperparameter optimization (HPO) can aid in exploring solutions. This study presents a comparative analysis of several strategies for Hyperparameter optimization of GNNs. The explored optimization techniques include complex algorithms such as the bio-inspired Genetic Algorithm, Particle Swarm Optimization, and Artificial Bee Colony. In addition, Hill Climb and Simulated Annealing as well as the commonly used methods Random Search and Bayesian Search have also been covered. The proposed optimization algorithms have been evaluated on improving the performance of the GNN architectures developed for predicting mRNA degradation. The Stanford OpenVaccine dataset for mRNA degradation prediction has been used for training and testing the predictive models. Finding mRNA molecules with low degradation rates is important in development of mRNA vaccines for diseases such as COVID-19 and we hope to benefit research on ML in this domain. According to the analysisâ€™s findings, Simulated Annealing algorithm outperforms other algorithms on both architectures. Furthermore, population based algorithms like Particle Swarm optimization show promising results, with certain limitations related to the complexity of the algorithms which encourages further exploration of the subject.},
	urldate = {2025-10-20},
	booktitle = {2023 46th {MIPRO} {ICT} and {Electronics} {Convention} ({MIPRO})},
	author = {Vodilovska, Viktorija and Gievska, Sonja and Ivanoska, Ilinka},
	month = may,
	year = {2023},
	note = {ISSN: 2623-8764},
	keywords = {Artificial Bee Colony, Bayesian search, Degradation, GAT, GCN, Genetic Algorithm, Graph neural networks, Hill Climbing, Hyperparameter optimization, Hyperparameter Optimization, mRNA degradation, mRNA vaccines, Particle Swarm Optimization, Prediction algorithms, Random Search, Simulated annealing, Simulated Annealing, Sociology, Surveys},
	pages = {423--428},
}
