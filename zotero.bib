
@article{rozemberczki_multi-scale_2021,
	title = {Multi-{Scale} attributed node embedding},
	volume = {9},
	issn = {2051-1329},
	url = {https://doi.org/10.1093/comnet/cnab014},
	doi = {10.1093/comnet/cnab014},
	abstract = {We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.},
	number = {2},
	urldate = {2022-11-28},
	journal = {Journal of Complex Networks},
	author = {Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},
	month = apr,
	year = {2021},
	pages = {cnab014},
}

@inproceedings{bojchevski_deep_2018,
	title = {Deep {Gaussian} {Embedding} of {Graphs}: {Unsupervised} {Inductive} {Learning} via {Ranking}},
	shorttitle = {Deep {Gaussian} {Embedding} of {Graphs}},
	url = {https://openreview.net/forum?id=r1ZdKJ-0W},
	abstract = {We embed nodes in a graph as Gaussian distributions allowing us to capture uncertainty about their representation.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}},
	author = {Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
	month = feb,
	year = {2018},
}

@inproceedings{chen_fastgcn_2018,
	title = {{FastGCN}: {Fast} {Learning} with {Graph} {Convolutional} {Networks} via {Importance} {Sampling}},
	shorttitle = {{FastGCN}},
	url = {https://openreview.net/forum?id=rytstxWAW},
	abstract = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because...},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}},
	author = {Chen, Jie and Ma, Tengfei and Xiao, Cao},
	month = feb,
	year = {2018},
}

@inproceedings{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	booktitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	year = {2017},
	pages = {1024--1034},
	file = {Full Text:/home/marekdedic/Zotero/storage/5Y9Q2N8U/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf;Snapshot:/home/marekdedic/Zotero/storage/B8EPL4ZN/6703-inductive-representation-learning-on-large-graphs.html:text/html},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/marekdedic/Zotero/storage/NQUH3G63/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges.pdf:application/pdf;Snapshot:/home/marekdedic/Zotero/storage/C2NAGDJD/2104.html:text/html},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	urldate = {2024-06-14},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
	year = {2011},
	pages = {2825--2830},
	file = {Full Text PDF:/home/marekdedic/Zotero/storage/7BUKH7K2/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@misc{shchur_pitfalls_2019,
	title = {Pitfalls of {Graph} {Neural} {Network} {Evaluation}},
	url = {http://arxiv.org/abs/1811.05868},
	doi = {10.48550/arXiv.1811.05868},
	abstract = {Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
	month = jun,
	year = {2019},
	note = {arXiv:1811.05868 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@inproceedings{yang_diverse_2021,
	title = {Diverse {Message} {Passing} for {Attribute} with {Heterophily}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/253614bbac999b38b5b60cae531c4969-Abstract.html},
	abstract = {Most of the existing GNNs can be modeled via the Uniform Message Passing framework. This framework considers all the attributes of each node in its entirety, shares the uniform propagation weights along each edge,  and focuses on the uniform weight learning. The design of this framework possesses two prerequisites, the simplification of homophily and heterophily to the node-level property and the ignorance of attribute differences. Unfortunately, different attributes possess diverse characteristics. In this paper, the network homophily rate defined with  respect to the node labels is extended to attribute homophily rate by taking the attributes as weak labels. Based on this attribute homophily rate, we propose a Diverse Message Passing (DMP) framework, which specifies every attribute propagation weight on each edge. Besides, we propose two specific strategies to significantly reduce the computational complexity of DMP to prevent the overfitting issue.  By investigating the spectral characteristics, existing spectral GNNs are actually equivalent to a degenerated version of DMP.  From the perspective of numerical optimization, we provide a theoretical analysis to demonstrate DMP's powerful representation ability and the ability of alleviating the over-smoothing issue.  Evaluations on various  real networks demonstrate the superiority of our DMP on  handling the networks with heterophily  and alleviating the over-smoothing issue, compared to the existing state-of-the-arts.},
	urldate = {2023-06-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Liang and Li, Mengzhe and Liu, Liyang and niu, bingxin and Wang, Chuan and Cao, Xiaochun and Guo, Yuanfang},
	year = {2021},
	pages = {4751--4763},
}

@inproceedings{yang_revisiting_2016,
	address = {New York, NY, USA},
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {https://proceedings.mlr.press/v48/yanga16.html},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
	month = jun,
	year = {2016},
	pages = {40--48},
}

@inproceedings{zeng_graphsaint_2019,
	title = {{GraphSAINT}: {Graph} {Sampling} {Based} {Inductive} {Learning} {Method}},
	shorttitle = {{GraphSAINT}},
	url = {https://openreview.net/forum?id=BJe8pkHFwS},
	abstract = {Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the "neighbor explosion" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).},
	language = {en},
	urldate = {2023-06-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zeng, Hanqing and Zhou, Hongkuan and Srivastava, Ajitesh and Kannan, Rajgopal and Prasanna, Viktor},
	month = sep,
	year = {2019},
}

@misc{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	doi = {10.48550/arXiv.2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv:2005.00687 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: Fix dataset bug in ogbg-code},
	annote = {Comment: Fix dataset bug in ogbg-code},
}

@inproceedings{lim_large_2021,
	address = {online},
	title = {Large {Scale} {Learning} on {Non}-{Homophilous} {Graphs}: {New} {Benchmarks} and {Strong} {Simple} {Methods}},
	volume = {34},
	shorttitle = {Large {Scale} {Learning} on {Non}-{Homophilous} {Graphs}},
	url = {\url{https://proceedings.neurips.cc/paper/2021/file/ae816a80e4c1c56caa2eb4e1819cbb2f-Paper.pdf}},
	abstract = {Many widely used datasets for graph machine learning tasks have generally been homophilous, where nodes with similar labels connect to each other. Recently, new Graph Neural Networks (GNNs) have been developed that move beyond the homophily regime; however, their evaluation has often been conducted on small graphs with limited application domains. We collect and introduce diverse non-homophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets. We further show that existing scalable graph learning and graph minibatching techniques lead to performance degradation on these non-homophilous datasets, thus highlighting the need for further work on scalable non-homophilous methods. To address these concerns, we introduce LINKX --- a strong simple method that admits straightforward minibatch training and inference. Extensive experimental results with representative simple methods and GNNs across our proposed datasets show that LINKX achieves state-of-the-art performance for learning on non-homophilous graphs. Our codes and data are available at https://github.com/CUAI/Non-Homophily-Large-Scale.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lim, Derek and Hohne, Felix and Li, Xiuyu and Huang, Sijia Linda and Gupta, Vaishnavi and Bhalerao, Omkar and Lim, Ser Nam},
	year = {2021},
	pages = {20887--20902},
}

@inproceedings{zhu_beyond_2020,
	address = {online},
	title = {Beyond {Homophily} in {Graph} {Neural} {Networks}: {Current} {Limitations} and {Effective} {Designs}},
	volume = {33},
	shorttitle = {Beyond {Homophily} in {Graph} {Neural} {Networks}},
	url = {\url{https://proceedings.neurips.cc/paper/2020/file/58ae23d878a47004366189884c2f8440-Paper.pdf}},
	abstract = {We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs{\textemdash}ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations{\textemdash}that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H2GCN, which we use as the base method to empirically evaluate the effectiveness of the identified designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identified designs increase the accuracy of GNNs by up to 40\% and 27\% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
	year = {2020},
	pages = {7793--7804},
}

@article{newman_mixing_2003,
	title = {Mixing patterns in networks},
	volume = {67},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.67.026126},
	doi = {10.1103/PhysRevE.67.026126},
	abstract = {We study assortative mixing in networks, the tendency for vertices in networks to be connected to other vertices that are like (or unlike) them in some way. We consider mixing according to discrete characteristics such as language or race in social networks and scalar characteristics such as age. As a special example of the latter we consider mixing according to vertex degree, i.e., according to the number of connections vertices have to other vertices: do gregarious people tend to associate with other gregarious people? We propose a number of measures of assortative mixing appropriate to the various mixing types, and apply them to a variety of real-world networks, showing that assortative mixing is a pervasive phenomenon found in many networks. We also propose several models of assortatively mixed networks, both analytic ones based on generating function methods, and numerical ones based on Monte Carlo graph generation techniques. We use these models to probe the properties of networks as their level of assortativity is varied. In the particular case of mixing by degree, we find strong variation with assortativity in the connectivity of the network and in the resilience of the network to the removal of vertices.},
	number = {2},
	urldate = {2022-11-28},
	journal = {Physical Review E},
	author = {Newman, M. E. J.},
	month = feb,
	year = {2003},
	note = {Publisher: American Physical Society},
	pages = {026126},
}

@misc{pei_geom-gcn_2020,
	title = {Geom-{GCN}: {Geometric} {Graph} {Convolutional} {Networks}},
	shorttitle = {Geom-{GCN}},
	url = {http://arxiv.org/abs/2002.05287},
	doi = {10.48550/arXiv.2002.05287},
	abstract = {Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Pei, Hongbin and Wei, Bingzhe and Chang, Kevin Chen-Chuan and Lei, Yu and Yang, Bo},
	month = feb,
	year = {2020},
	note = {arXiv:2002.05287 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2020},
}

@article{ebadi_hyperparameter_2025,
	title = {Hyperparameter optimization and neural architecture search algorithms for graph {Neural} {Networks} in cheminformatics},
	volume = {254},
	issn = {0927-0256},
	url = {https://www.sciencedirect.com/science/article/pii/S0927025625002472},
	doi = {10.1016/j.commatsci.2025.113904},
	abstract = {Cheminformatics, an interdisciplinary field bridging chemistry and information science, leverages computational tools to analyze and interpret chemical data, playing a critical role in drug discovery, material science, and environmental chemistry. Traditional methods, reliant on rule-based algorithms and expert-curated datasets, face challenges in scalability and adaptability. Recently, machine learning and deep learning have revolutionized cheminformatics by offering data-driven approaches that uncover complex patterns in vast chemical datasets, advancing molecular property prediction, chemical reaction modeling, and de novo molecular design. Among the most promising techniques are Graph Neural Networks (GNNs), which have emerged as a powerful tool for modeling molecules in a manner that mirrors their underlying chemical structures. Despite their success, the performance of GNNs is highly sensitive to architectural choices and hyperparameters, making optimal configuration selection a non-trivial task. Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) are crucial for improving GNN performance, but the complexity and computational cost of these processes have traditionally hindered progress. This review examines various strategies for automating NAS and HPO in GNNs, highlighting their potential to enhance model performance, scalability, and efficiency in key cheminformatics applications. As the field evolves, automated optimization techniques are expected to play a pivotal role in advancing GNN-based solutions in cheminformatics.},
	urldate = {2025-10-20},
	journal = {Computational Materials Science},
	author = {Ebadi, Ali and Kaur, Manpreet and Liu, Qian},
	month = may,
	year = {2025},
	keywords = {Cheminformatics, Deep learning (DL), Graph neural networks (GNN), Hyperparameter optimization (HPO), Machine learning (ML), Neural architecture search (NAS)},
	pages = {113904},
}

@inproceedings{yuan_which_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {Which hyperparameters to optimise? an investigation of evolutionary hyperparameter optimisation in graph neural network for molecular property prediction},
	isbn = {978-1-4503-8351-6},
	shorttitle = {Which hyperparameters to optimise?},
	url = {https://doi.org/10.1145/3449726.3463192},
	doi = {10.1145/3449726.3463192},
	abstract = {Most GNNs for molecular property prediction are proposed based on the idea of learning the representations for the nodes by aggregating the information of their neighbour nodes in graph layers. Then, the representations can be passed to subsequent task-specific layers to deal with individual downstream tasks. Facing real-world molecular problems, the hyperparameter optimisation for those layers are vital. In this research, we focus on the impact of selecting two types of GNN hyperparameters, those belonging to graph layers and those of task-specific layers, on the performance of GNN for molecular property prediction. In our experiments, we employed a state-of-the-art evolutionary algorithm (i.e., CMA-ES) for HPO. The results reveal that optimising the two types of hyperparameters separately can improve GNNs' performance, but optimising both types of hyperparameters simultaneously will lead to predominant improvements.},
	urldate = {2025-10-20},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jul,
	year = {2021},
	pages = {1403--1404},
}

@inproceedings{yuan_genetic_2021,
	title = {A {Genetic} {Algorithm} with {Tree}-structured {Mutation} for {Hyperparameter} {Optimisation} of {Graph} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/abstract/document/9504717},
	doi = {10.1109/CEC45853.2021.9504717},
	abstract = {In recent years, graph neural networks (GNNs) have gained increasing attention, as they possess the excellent capability of processing graph-related problems. In practice, hyperparameter optimisation (HPO) is critical for GNNs to achieve satisfactory results, but this process is costly because the evaluations of different hyperparameter settings require excessively training many GNNs. Many approaches have been proposed for HPO, which aims to identify promising hyperparameters efficiently. In particular, the genetic algorithm (GA) for HPO has been explored, which treats GNNs as a black-box model, of which only the outputs can be observed given a set of hyperparameters. However, because GNN models are sophisticated and the evaluations of hyperparameters on GNNs are expensive, GA requires advanced techniques to balance the exploration and exploitation of the search and make the optimisation more effective given limited computational resources. Therefore, we proposed a tree-structured mutation strategy for GA to alleviate this issue. Meanwhile, we reviewed the recent HPO works, which gives room for the idea of tree-structure to develop, and we hope our approach can further improve these HPO methods in the future.},
	urldate = {2025-10-20},
	booktitle = {2021 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jun,
	year = {2021},
	keywords = {Training, graph neural network, Graph neural networks, Computational modeling, hyperparameter optimisation, Evolutionary computation, generic algorithm, Genetic algorithms, Optimization, tree-structured mutation},
	pages = {482--489},
}

@inproceedings{yuan_systematic_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {A systematic comparison study on hyperparameter optimisation of graph neural networks for molecular property prediction},
	isbn = {978-1-4503-8350-9},
	url = {https://dl.acm.org/doi/10.1145/3449639.3459370},
	doi = {10.1145/3449639.3459370},
	abstract = {Graph neural networks (GNNs) have been proposed for a wide range of graph-related learning tasks. In particular, in recent years, an increasing number of GNN systems were applied to predict molecular properties. However, a direct impediment is to select appropriate hyperparameters to achieve satisfactory performance with lower computational cost. Meanwhile, many molecular datasets are far smaller than many other datasets in typical deep learning applications. Most hyperparameter optimization (HPO) methods have not been explored in terms of their efficiencies on such small datasets in the molecular domain. In this paper, we conducted a theoretical analysis of common and specific features for two state-of-the-art and popular algorithms for HPO: TPE and CMA-ES, and we compared them with random search (RS), which is used as a baseline. Experimental studies are carried out on several benchmarks in MoleculeNet, from different perspectives to investigate the impact of RS, TPE, and CMA-ES on HPO of GNNs for molecular property prediction. In our experiments, we concluded that RS, TPE, and CMA-ES have their individual advantages in tackling different specific molecular problems. Finally, we believe our work will motivate further research on GNN as applied to molecular machine learning problems in chemistry and materials sciences.},
	urldate = {2025-10-20},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jun,
	year = {2021},
	pages = {386--394},
}

@inproceedings{vodilovska_hyperparameter_2023,
	title = {Hyperparameter {Optimization} of {Graph} {Neural} {Networks} for {mRNA} {Degradation} {Prediction}},
	url = {https://ieeexplore.ieee.org/abstract/document/10159737},
	doi = {10.23919/MIPRO57284.2023.10159737},
	abstract = {Graph Neural Networks (GNN) emerged as increasingly attractive deep learning models for complex data, making them extremely useful in biochemical and pharmaceutical domains. However, building a good-performing GNN requires lots of parameter choices and Hyperparameter optimization (HPO) can aid in exploring solutions. This study presents a comparative analysis of several strategies for Hyperparameter optimization of GNNs. The explored optimization techniques include complex algorithms such as the bio-inspired Genetic Algorithm, Particle Swarm Optimization, and Artificial Bee Colony. In addition, Hill Climb and Simulated Annealing as well as the commonly used methods Random Search and Bayesian Search have also been covered. The proposed optimization algorithms have been evaluated on improving the performance of the GNN architectures developed for predicting mRNA degradation. The Stanford OpenVaccine dataset for mRNA degradation prediction has been used for training and testing the predictive models. Finding mRNA molecules with low degradation rates is important in development of mRNA vaccines for diseases such as COVID-19 and we hope to benefit research on ML in this domain. According to the analysis{\textquoteright}s findings, Simulated Annealing algorithm outperforms other algorithms on both architectures. Furthermore, population based algorithms like Particle Swarm optimization show promising results, with certain limitations related to the complexity of the algorithms which encourages further exploration of the subject.},
	urldate = {2025-10-20},
	booktitle = {2023 46th {MIPRO} {ICT} and {Electronics} {Convention} ({MIPRO})},
	author = {Vodilovska, Viktorija and Gievska, Sonja and Ivanoska, Ilinka},
	month = may,
	year = {2023},
	note = {ISSN: 2623-8764},
	keywords = {Prediction algorithms, Graph neural networks, Artificial Bee Colony, Bayesian search, Degradation, GAT, GCN, Genetic Algorithm, Hill Climbing, Hyperparameter optimization, Hyperparameter Optimization, mRNA degradation, mRNA vaccines, Particle Swarm Optimization, Random Search, Simulated annealing, Simulated Annealing, Sociology, Surveys},
	pages = {423--428},
}

@article{bischl_hyperparameter_2023,
	title = {Hyperparameter optimization: {Foundations}, algorithms, best practices, and open challenges},
	volume = {13},
	copyright = {{\textcopyright} 2023 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.},
	issn = {1942-4795},
	shorttitle = {Hyperparameter optimization},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1484},
	doi = {10.1002/widm.1484},
	abstract = {Most machine learning algorithms are configured by a set of hyperparameters whose values must be carefully chosen and which often considerably impact performance. To avoid a time-consuming and irreproducible manual process of trial-and-error to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods{\textemdash}for example, based on resampling error estimation for supervised machine learning{\textemdash}can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods, from simple techniques such as grid or random search to more advanced methods like evolution strategies, Bayesian optimization, Hyperband, and racing. This work gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with machine learning pipelines, runtime improvements, and parallelization. This article is categorized under: Algorithmic Development {\textgreater} Statistics Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction},
	language = {en},
	number = {2},
	urldate = {2025-10-21},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	year = {2023},
	note = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1484},
	keywords = {machine learning, automl, hyperparameter optimization, model selection, tuning},
	pages = {e1484},
}

@inproceedings{bergstra_algorithms_2011,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	volume = {24},
	url = {https://proceedings.neurips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
	abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [Larochelle et al., 2007] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
	urldate = {2025-10-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
	year = {2011},
}

@article{sobol_distribution_1967,
	title = {The distribution of points in a cube and the accurate evaluation of integrals (in {Russian}) {Zh}},
	volume = {7},
	journal = {Vychisl. Mat. i Mater. Phys},
	author = {Sobol, I.},
	year = {1967},
	pages = {784--802},
}

@article{bergstra_random_2012,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	volume = {13},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v13/bergstra12a.html},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	number = {10},
	urldate = {2025-10-21},
	journal = {Journal of Machine Learning Research},
	author = {Bergstra, James and Bengio, Yoshua},
	year = {2012},
	pages = {281--305},
}

@incollection{rasmussen_gaussian_2003,
	address = {Berlin, Heidelberg},
	title = {Gaussian {Processes} in {Machine} {Learning}},
	isbn = {978-3-540-28650-9},
	url = {https://doi.org/10.1007/978-3-540-28650-9_4},
	abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
	language = {en},
	urldate = {2025-10-21},
	booktitle = {Advanced {Lectures} on {Machine} {Learning}: {ML} {Summer} {Schools} 2003, {Canberra}, {Australia}, {February} 2 - 14, 2003, {T{\"u}bingen}, {Germany}, {August} 4 - 16, 2003, {Revised} {Lectures}},
	publisher = {Springer},
	author = {Rasmussen, Carl Edward},
	editor = {Bousquet, Olivier and von Luxburg, Ulrike and R{\"a}tsch, Gunnar},
	year = {2003},
	doi = {10.1007/978-3-540-28650-9_4},
	keywords = {Marginal Likelihood, Covariance Function, Gaussian Process, Joint Gaussian Distribution, Posterior Variance},
	pages = {63--71},
}

@inproceedings{akiba_optuna_2019,
	title = {Optuna: {A} next-generation hyperparameter optimization framework},
	doi = {10.1145/3292500.3330701},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} international conference on knowledge discovery \& data mining},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	year = {2019},
	pages = {2623--2631},
}

@article{matousek_l2-discrepancy_1998,
	title = {On the {L2}-discrepancy for anchored boxes.},
	volume = {14},
	url = {https://www.sciencedirect.com/science/article/pii/S0885064X98904897},
	number = {4},
	urldate = {2025-10-21},
	journal = {Journal of Complexity},
	author = {Matou{\v s}ek, Ji{\v r}{\i}?},
	year = {1998},
	note = {Publisher: Elsevier},
	pages = {527--556},
}

@inproceedings{falkner_bohb_2018,
	title = {{BOHB}: {Robust} and {Efficient} {Hyperparameter} {Optimization} at {Scale}},
	shorttitle = {{BOHB}},
	url = {https://proceedings.mlr.press/v80/falkner18a.html},
	abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
	language = {en},
	urldate = {2025-10-21},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1437--1446},
}

@inproceedings{yang_vqgraph_2023,
	title = {{VQGraph}: {Rethinking} {Graph} {Representation} {Space} for {Bridging} {GNNs} and {MLPs}},
	shorttitle = {{VQGraph}},
	url = {https://openreview.net/forum?id=h6Tz85BqRI},
	abstract = {GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828{\texttimes}, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90\% and 28.05\% on average, respectively. Our code is available at https://github.com/YangLing0818/VQGraph},
	language = {en},
	urldate = {2025-10-21},
	author = {Yang, Ling and Tian, Ye and Xu, Minkai and Liu, Zhongyi and Hong, Shenda and Qu, Wei and Zhang, Wentao and Cui, Bin and Zhang, Muhan and Leskovec, Jure},
	month = oct,
	year = {2023},
}

@inproceedings{li_finding_2022,
	title = {Finding {Global} {Homophily} in {Graph} {Neural} {Networks} {When} {Meeting} {Heterophily}},
	url = {https://proceedings.mlr.press/v162/li22ad.html},
	abstract = {We investigate graph neural networks on graphs with heterophily. Some existing methods amplify a node{\textquoteright}s neighborhood with multi-hop neighbors to include more nodes with homophily. However, it is a significant challenge to set personalized neighborhood sizes for different nodes. Further, for other homophilous nodes excluded in the neighborhood, they are ignored for information aggregation. To address these problems, we propose two models GloGNN and GloGNN++, which generate a node{\textquoteright}s embedding by aggregating information from global nodes in the graph. In each layer, both models learn a coefficient matrix to capture the correlations between nodes, based on which neighborhood aggregation is performed. The coefficient matrix allows signed values and is derived from an optimization problem that has a closed-form solution. We further accelerate neighborhood aggregation and derive a linear time complexity. We theoretically explain the models{\textquoteright} effectiveness by proving that both the coefficient matrix and the generated node embedding matrix have the desired grouping effect. We conduct extensive experiments to compare our models against 11 other competitors on 15 benchmark datasets in a wide range of domains, scales and graph heterophilies. Experimental results show that our methods achieve superior performance and are also very efficient.},
	language = {en},
	urldate = {2025-10-21},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Xiang and Zhu, Renyu and Cheng, Yao and Shan, Caihua and Luo, Siqiang and Li, Dongsheng and Qian, Weining},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {13242--13256},
}

@inproceedings{den_boef_graphpope_2021,
	title = {{GraphPOPE}: {Retaining} {Structural} {Graph} {Information} {Using} {Position}-aware {Node} {Embeddings}.},
	shorttitle = {{GraphPOPE}},
	url = {https://ceur-ws.org/Vol-3034/paper3.pdf},
	urldate = {2025-10-21},
	booktitle = {{DL4KG}@ {ISWC}},
	author = {den Boef, Jeroen and Cornelisse, Joran and Groth, Paul},
	year = {2021},
}

@article{zachary_information_1977,
	title = {An {Information} {Flow} {Model} for {Conflict} and {Fission} in {Small} {Groups}},
	volume = {33},
	issn = {0091-7710},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752},
	doi = {10.1086/jar.33.4.3629752},
	abstract = {Data from a voluntary association are used to construct a new formal model for a traditional anthropological problem, fission in small groups. The process leading to fission is viewed as an unequal flow of sentiments and information across the ties in a social network. This flow is unequal because it is uniquely constrained by the contextual range and sensitivity of each relationship in the network. The subsequent differential sharing of sentiments leads to the formation of subgroups with more internal stability than the group as a whole, and results in fission. The Ford-Fulkerson labeling algorithm allows an accurate prediction of membership in the subgroups and of the locus of the fission to be made from measurements of the potential for information flow across each edge in the network. Methods for measurement of potential information flow are discussed, and it is shown that all appropriate techniques will generate the same predictions.},
	number = {4},
	urldate = {2025-10-21},
	journal = {Journal of Anthropological Research},
	author = {Zachary, Wayne W.},
	month = dec,
	year = {1977},
	note = {Publisher: The University of Chicago Press},
	pages = {452--473},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148{\textendash}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2025-10-21},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {classification, ensemble, regression},
	pages = {5--32},
}
